{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{')': 17,\n",
       " '.': 13,\n",
       " 'BOS': 0,\n",
       " 'Bri\\\\u00fain': 24,\n",
       " 'Deshalb': 19,\n",
       " 'EOS': 4,\n",
       " 'Fiachrach': 22,\n",
       " 'Florida': 12,\n",
       " 'Lacus': 1,\n",
       " 'N\\\\u00e9ill': 25,\n",
       " 'S\\\\u00edl': 26,\n",
       " 'Siehe': 14,\n",
       " 'U\\\\u00ed': 21,\n",
       " '\\\\u00b7': 23,\n",
       " '\\\\u03a0\\\\u03bb\\\\u03b1\\\\u03c4\\\\u03b5\\\\u03af\\\\u03b1': 6,\n",
       " '\\\\u03c3\\\\u03c4\\\\u03b7\\\\u03bd': 5,\n",
       " '\\\\u03c4\\\\u03bf\\\\u03c5': 7,\n",
       " '\\\\u0421\\\\u0438\\\\u0446\\\\u0437\\\\u044f\\\\u043d': 16,\n",
       " '\\\\u0440\\\\u0430\\\\u0441\\\\u043f\\\\u043e\\\\u043b\\\\u043e\\\\u0436\\\\u0435\\\\u043d': 18,\n",
       " '\\\\u05dc\\\\u05d7\\\\u05e9\\\\u05d5\\\\u05d1': 10,\n",
       " '\\\\u05de\\\\u05e4\\\\u05e0\\\\u05d9': 8,\n",
       " '\\\\u05e9\\\\u05d8\\\\u05e2\\\\u05d5': 9,\n",
       " '\\\\u2022': 11,\n",
       " 'auch': 15,\n",
       " 'class': 2,\n",
       " 'fames': 3,\n",
       " 'wird': 20}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_dataset_x(fname):\n",
    "    f=open(fname, encoding=\"utf-8\")\n",
    "    dataset=[]\n",
    "    vpointer=0\n",
    "    vocabulary={}\n",
    "    for line in f:\n",
    "        line=line.replace(\"{\",\"\")\n",
    "        line=line.replace(\"}\",\"\")\n",
    "        line=line.split(\":\")[1]\n",
    "        line=line.replace(\"\\\"\",\"\")\n",
    "        words=line.split()\n",
    "        words.insert(0,\"BOS\")\n",
    "        words.insert(len(words),\"EOS\")\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word]=vpointer\n",
    "                vpointer+=1\n",
    "        words=[vocabulary[word] for word in words]\n",
    "        dataset.append(words)\n",
    "    return dataset,vocabulary\n",
    "def read_dataset_y(fname):\n",
    "    f=open(fname, encoding=\"utf-8\")\n",
    "    dataset=[]\n",
    "    vpointer=0\n",
    "    vocabulary={}\n",
    "    for line in f:\n",
    "        line=line.replace(\"{\",\"\")\n",
    "        line=line.replace(\"}\",\"\")\n",
    "        line=line.split(\":\")[1]\n",
    "        line=line.replace(\"\\\"\",\"\")\n",
    "        words=line.split()\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word]=vpointer\n",
    "                vpointer+=1\n",
    "        words=[vocabulary[word] for word in words]\n",
    "        dataset.append(words)\n",
    "    return dataset,vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextIterator:\n",
    "    def __init__(self,x_dataset,y_dataset,batch_size=20):\n",
    "        self.x=x_dataset\n",
    "        self.y=y_dataset\n",
    "        self.n=len(x_dataset)\n",
    "        self.indices=[i for i in range(self.n)]\n",
    "        random.shuffle(self.indices)\n",
    "        self.current_index=0\n",
    "        self.batch_size=batch_size\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def reset(self):\n",
    "        self.current_index=0\n",
    "    def get_elements_from_index(self,indices):\n",
    "        lx=[]\n",
    "        ly=[]\n",
    "        for i in indices:\n",
    "            lx.append(self.x[i]) \n",
    "            ly.append(self.y[i])\n",
    "        return lx,ly\n",
    "    def __next__(self):\n",
    "        if self.current_index==-1:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "        if (self.current_index+self.batch_size)<self.n:\n",
    "            indices=self.indices[self.current_index:self.current_index+self.batch_size]\n",
    "            x,y=self.get_elements_from_index(indices)\n",
    "            self.current_index+=self.batch_size\n",
    "            if self.current_index==self.n:\n",
    "                self.reset()\n",
    "                raise StopIteration\n",
    "        else:\n",
    "            size=self.n-self.current_index\n",
    "            indices=self.indices[self.current_index:self.current_index+size]\n",
    "            x,y=self.get_elements_from_index(indices)\n",
    "            self.current_index=-1\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_=read_dataset('data/sample.txt')\n",
    "y,_=read_dataset('data/sampley.txt')\n",
    "t=TextIterator(x,y,4)\n",
    "for log,lab in t:\n",
    "    print(log,lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_dataset(fname,vocabulary):\n",
    "    f=open(fname, encoding=\"utf-8\")\n",
    "    dataset=[]\n",
    "    cnt1=0\n",
    "    cnt2=0\n",
    "    for line in f:\n",
    "        sent=[]\n",
    "        line=line.replace(\"{\",\"\")\n",
    "        line=line.replace(\"}\",\"\")\n",
    "        line=line.split(\":\")[1]\n",
    "        line=line.replace(\"\\\"\",\"\")\n",
    "        words=line.split()\n",
    "        for word in words:\n",
    "            if word in v.keys():\n",
    "                sent.append(vocabulary[word])\n",
    "            else:\n",
    "                sent.append(vocabulary[\"UNK\"])\n",
    "        dataset.append(sent)\n",
    "    return dataset\n",
    "#read_test_dataset(\"data/test_X_languages_homework.json.txt\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extras(vocabulary):\n",
    "    n=len(vocabulary)\n",
    "    vocabulary[\"UNK\"]=n\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=add_unknown(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_feed(model,x,y,max_sen_len):\n",
    "    n_sen=len(x)\n",
    "    for i in range(n_sen):\n",
    "        if len(x[i])>max_sen_len:\n",
    "            x.pop(i)\n",
    "            y.pop(i)\n",
    "    n_sen=len(x)\n",
    "    seq_len_x=[len(xi) for xi in x]\n",
    "    max_len_x=max(seq_len_x)\n",
    "    x_arr=np.zeros(n_sen,max_len_x)\n",
    "    x_mask=np.zeros(n_sen,max_len_x)\n",
    "    for i,xi in enumerate(x):\n",
    "        for j,xj in enumerate(xi):\n",
    "            x_arr[i][j]=xj\n",
    "            x_mask[i][j]=1\n",
    "    return {\n",
    "        model.x_inputs: x_arr,\n",
    "        model.x_mask: x_mask,\n",
    "        model.y: y,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
