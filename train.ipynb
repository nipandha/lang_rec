{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 13, 14, 15, 4], [0, 21, 22, 23, 21, 24, 23, 21, 25, 23, 26, 4], [0, 16, 17, 18, 4], [0, 8, 9, 10, 4]] [[0, 6, 2], [0, 8, 2], [0, 7, 2], [0, 4, 2]]\n",
      "[[0, 13, 19, 20, 4], [0, 11, 12, 11, 4], [0, 5, 6, 7, 4], [0, 1, 2, 3, 4]] [[0, 6, 2], [0, 5, 2], [0, 3, 2], [0, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import yaml\n",
    "from utils import read_dataset,read_test_dataset,TextIterator,add_extras,prepare_input_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''parser = argparse.ArgumentParser(description='Train model')\n",
    "parser.add_argument('--fname', type=str, default='fname', metavar='F',\n",
    "                    help=\"image to be masked,output stored at name_mask.jpg\")\n",
    "args = parser.parse_args()\n",
    "timestr = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "log_file=log_dir+\"//log\"+timestr+\".txt\"\n",
    "dtype = tf.float16 # else tf.float32\n",
    "\n",
    "test_path=cfg['test_path']\n",
    "emb_dim=cfg['emb_dim']\n",
    "src_embed_path=cfg['src_embed_path']\n",
    "target_embed_path=cfg['target_embed_path']\n",
    "max_gradient_norm=cfg['max_gradient_norm']\n",
    "min_learning_rate=cfg['min_learning_rate']\n",
    "data_dir=cfg['data_dir']\n",
    "source_train_path=cfg['source_train_path']\n",
    "target_train_path=cfg['target_train_path']\n",
    "source_dev_path=cfg['source_dev_path']\n",
    "target_dev_path=cfg['target_dev_path']\n",
    "ref_path=cfg['ref_path']\n",
    "graph_path=cfg['graph_path']\n",
    "checkpoint_dir=cfg['checkpoint_dir']\n",
    "num_layers=cfg['num_layers']\n",
    "train_dir=cfg['train_dir']\n",
    "batch_size=cfg['batch_size']\n",
    "learning_rate=cfg['learning_rate']\n",
    "learning_rate_decay_factor=cfg['learning_rate_decay_factor']\n",
    "max_train_data_size=cfg['max_train_data_size']\n",
    "steps_per_checkpoint=cfg['steps_per_checkpoint']\n",
    "num_epochs=cfg['num_epochs']\n",
    "keep_prob=cfg['keep_prob']\n",
    "\n",
    "embed_dir=\"embed\"\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"translate.ckpt\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(session, forward_only,src_embeddings=None,target_embeddings=None):\n",
    "    #Use forward only for decode\n",
    "    \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "    model = LID_model()\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Created model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    flog = open(log_file, 'a')\n",
    "    x_dataset,v=read_dataset_x(x_train_path)\n",
    "    y_dataset,classes=read_dataset(y_train_path)\n",
    "    v=add_extras(v)\n",
    "    n=len(x_dataset)\n",
    "    n1=int(0.8*n)\n",
    "    train_iterator=TextIterator(x_dataset[0:n1],y_dataset[0:n1],batch_size)\n",
    "    val_iterator=TextIterator(x_dataset[n1:],y_dataset[n1:],batch_size)\n",
    "    with tf.Session() as sess:\n",
    "    # Create model.\n",
    "        \n",
    "        model = create_model(sess, False)\n",
    "\n",
    "        # This is the training loop.\n",
    "        valid_acc=[]\n",
    "        max_acc=0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "\n",
    "            i=0\n",
    "            for batch_X,batch_Y in train_iterator:\n",
    "                \n",
    "                start_time = time.time()\n",
    "                #Prepare data inplaceholder format\n",
    "                input_feed=data_utils.prepare_input_feed(model, batch_X, batch_Y,max_seq_len)\n",
    "\n",
    "                output_feed = [model.update,  # Update Op that does SGD.\n",
    "                              model.norm,  # Gradient norm.\n",
    "                              model.loss]  # Loss for this batch.\n",
    "                _,_,loss = sess.run(output_feed, input_feed)\n",
    "                print(\"Loss at batch \",i,loss)\n",
    "                \n",
    "            acc=0\n",
    "            n=0\n",
    "            for batch_X,batch_Y in val_iterator:\n",
    "                input_feed=data_utils.prepare_input_feed(model, batch_X, batch_Y,max_seq_len)\n",
    "                output_feed=[model.accuracy]\n",
    "                v=sess.run(output_feed, input_feed)\n",
    "                acc+=v\n",
    "                n+=1\n",
    "            acc/=n\n",
    "            print(\"Val acc at epoch \",epoch,acc)\n",
    "            #If average epoch loss stays constant for 3 epochs decrease learning rate\n",
    "            if len(valid_acc) > 2 and min(valid_acc[-3])>acc:\n",
    "                sess.run(model.learning_rate_decay_op)\n",
    "                lr=sess.run(model.learning_rate)\n",
    "                if lr<min_learning_rate:\n",
    "                    print(\"Reached convergence at epoch \" + str(epoch) + \"\\n\")\n",
    "                    break\n",
    "            valid_acc.append(acc)\n",
    "\n",
    "            #If average loss over the epoch has reduced, save checkpoint\n",
    "            if(acc > max_acc ):\n",
    "                model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "                print(\"Saved checkpoint\")\n",
    "               \n",
    "\n",
    "            #Store graph\n",
    "            writer = tf.summary.FileWriter(graph_path, sess.graph)\n",
    "            writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
