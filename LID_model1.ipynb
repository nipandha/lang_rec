{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import BaseModel\n",
    "\n",
    "\n",
    "class NERModel(BaseModel):\n",
    "    \"\"\"Specialized class of Model for NER\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(NERModel, self).__init__(config)\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Define placeholders = entries to computational graph\"\"\"\n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_ids\")\n",
    "\n",
    "        # shape = (batch size)\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                        name=\"sequence_lengths\")\n",
    "\n",
    "        '''# shape = (batch size, max length of sentence, max length of word)\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None],\n",
    "                        name=\"char_ids\")\n",
    "\n",
    "        # shape = (batch_size, max_length of sentence)\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_lengths\")'''\n",
    "\n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"labels\")\n",
    "\n",
    "        # hyper parameters\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"dropout\")\n",
    "        self.lr = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"lr\")\n",
    "\n",
    "\n",
    "    def get_feed_dict(self, words, labels=None, lr=None, dropout=None):\n",
    "        \"\"\"Given some data, pad it and build a feed dictionary\n",
    "\n",
    "        Args:\n",
    "            words: list of sentences. A sentence is a list of ids of a list of\n",
    "                words. A word is a list of ids\n",
    "            labels: list of ids\n",
    "            lr: (float) learning rate\n",
    "            dropout: (float) keep prob\n",
    "\n",
    "        Returns:\n",
    "            dict {placeholder: value}\n",
    "\n",
    "        \"\"\"\n",
    "        # perform padding of the given data\n",
    "        if self.config.use_chars:\n",
    "            char_ids, word_ids = zip(*words)\n",
    "            word_ids, sequence_lengths = pad_sequences(word_ids, 0)\n",
    "            char_ids, word_lengths = pad_sequences(char_ids, pad_tok=0,\n",
    "                nlevels=2)\n",
    "        else:\n",
    "            word_ids, sequence_lengths = pad_sequences(words, 0)\n",
    "\n",
    "        # build feed dictionary\n",
    "        feed = {\n",
    "            self.word_ids: word_ids,\n",
    "            self.sequence_lengths: sequence_lengths\n",
    "        }\n",
    "\n",
    "        if self.config.use_chars:\n",
    "            feed[self.char_ids] = char_ids\n",
    "            feed[self.word_lengths] = word_lengths\n",
    "\n",
    "        if labels is not None:\n",
    "            labels, _ = pad_sequences(labels, 0)\n",
    "            feed[self.labels] = labels\n",
    "\n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "\n",
    "        if dropout is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "\n",
    "        return feed, sequence_lengths\n",
    "\n",
    "\n",
    "    def add_word_embeddings_op(self):\n",
    "        \"\"\"Defines self.word_embeddings\n",
    "\n",
    "        If self.config.embeddings is not None and is a np array initialized\n",
    "        with pre-trained word vectors, the word embeddings is just a look-up\n",
    "        and we don't train the vectors. Otherwise, a random matrix with\n",
    "        the correct shape is initialized.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"words\"):\n",
    "            if self.config.embeddings is None:\n",
    "                self.logger.info(\"WARNING: randomly initializing word vectors\")\n",
    "                _word_embeddings = tf.get_variable(\n",
    "                        name=\"_word_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        shape=[self.config.nwords, self.config.dim_word])\n",
    "            else:\n",
    "                _word_embeddings = tf.Variable(\n",
    "                        self.config.embeddings,\n",
    "                        name=\"_word_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        trainable=self.config.train_embeddings)\n",
    "\n",
    "            word_embeddings = tf.nn.embedding_lookup(_word_embeddings,\n",
    "                    self.word_ids, name=\"word_embeddings\")\n",
    "\n",
    "        '''with tf.variable_scope(\"chars\"):\n",
    "            if self.config.use_chars:\n",
    "                # get char embeddings matrix\n",
    "                _char_embeddings = tf.get_variable(\n",
    "                        name=\"_char_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        shape=[self.config.nchars, self.config.dim_char])\n",
    "                char_embeddings = tf.nn.embedding_lookup(_char_embeddings,\n",
    "                        self.char_ids, name=\"char_embeddings\")\n",
    "\n",
    "                # put the time dimension on axis=1\n",
    "                s = tf.shape(char_embeddings)\n",
    "                char_embeddings = tf.reshape(char_embeddings,\n",
    "                        shape=[s[0]*s[1], s[-2], self.config.dim_char])\n",
    "                word_lengths = tf.reshape(self.word_lengths, shape=[s[0]*s[1]])\n",
    "\n",
    "                # bi lstm on chars\n",
    "                cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n",
    "                        state_is_tuple=True)\n",
    "                cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n",
    "                        state_is_tuple=True)\n",
    "                _output = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        cell_fw, cell_bw, char_embeddings,\n",
    "                        sequence_length=word_lengths, dtype=tf.float32)\n",
    "\n",
    "                # read and concat output\n",
    "                _, ((_, output_fw), (_, output_bw)) = _output\n",
    "                output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "\n",
    "                # shape = (batch size, max sentence length, char hidden size)\n",
    "                output = tf.reshape(output,\n",
    "                        shape=[s[0], s[1], 2*self.config.hidden_size_char])\n",
    "                word_embeddings = tf.concat([word_embeddings, output], axis=-1) '''\n",
    "\n",
    "        self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout)\n",
    "\n",
    "\n",
    "    def add_logits_op(self):\n",
    "        \"\"\"Defines self.logits\n",
    "\n",
    "        For each word in each sentence of the batch, it corresponds to a vector\n",
    "        of scores, of dimension equal to the number of tags.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw, cell_bw, self.word_embeddings,\n",
    "                    sequence_length=self.sequence_lengths, dtype=tf.float32)\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.nn.dropout(output, self.dropout)\n",
    "\n",
    "        with tf.variable_scope(\"proj\"):\n",
    "            W = tf.get_variable(\"W\", dtype=tf.float32,\n",
    "                    shape=[2*self.config.hidden_size_lstm, self.config.ntags])\n",
    "\n",
    "            b = tf.get_variable(\"b\", shape=[self.config.ntags],\n",
    "                    dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "            nsteps = tf.shape(output)[1]\n",
    "            output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm])\n",
    "            pred = tf.matmul(output, W) + b\n",
    "            self.logits = tf.reshape(pred, [-1, nsteps, self.config.ntags])\n",
    "\n",
    "\n",
    "    def add_pred_op(self):\n",
    "        \"\"\"Defines self.labels_pred\n",
    "\n",
    "        This op is defined only in the case where we don't use a CRF since in\n",
    "        that case we can make the prediction \"in the graph\" (thanks to tf\n",
    "        functions in other words). With theCRF, as the inference is coded\n",
    "        in python and not in pure tensroflow, we have to make the prediciton\n",
    "        outside the graph.\n",
    "        \"\"\"\n",
    "        if not self.config.use_crf:\n",
    "            self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1),\n",
    "                    tf.int32)\n",
    "\n",
    "\n",
    "    def add_loss_op(self):\n",
    "        \"\"\"Defines the loss\"\"\"\n",
    "        if self.config.use_crf:\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
    "                    self.logits, self.labels, self.sequence_lengths)\n",
    "            self.trans_params = trans_params # need to evaluate it for decoding\n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "        else:\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=self.labels)\n",
    "            mask = tf.sequence_mask(self.sequence_lengths)\n",
    "            losses = tf.boolean_mask(losses, mask)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        # for tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        # NER specific functions\n",
    "        self.add_placeholders()\n",
    "        self.add_word_embeddings_op()\n",
    "        self.add_logits_op()\n",
    "        self.add_pred_op()\n",
    "        self.add_loss_op()\n",
    "\n",
    "        # Generic functions that add training op and initialize session\n",
    "        self.add_train_op(self.config.lr_method, self.lr, self.loss,\n",
    "                self.config.clip)\n",
    "        self.initialize_session() # now self.sess is defined and vars are init\n",
    "\n",
    "\n",
    "    def predict_batch(self, words):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            words: list of sentences\n",
    "\n",
    "        Returns:\n",
    "            labels_pred: list of labels for each sentence\n",
    "            sequence_length\n",
    "\n",
    "        \"\"\"\n",
    "        fd, sequence_lengths = self.get_feed_dict(words, dropout=1.0)\n",
    "\n",
    "        if self.config.use_crf:\n",
    "            # get tag scores and transition params of CRF\n",
    "            viterbi_sequences = []\n",
    "            logits, trans_params = self.sess.run(\n",
    "                    [self.logits, self.trans_params], feed_dict=fd)\n",
    "\n",
    "            # iterate over the sentences because no batching in vitervi_decode\n",
    "            for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "                logit = logit[:sequence_length] # keep only the valid steps\n",
    "                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                        logit, trans_params)\n",
    "                viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "            return viterbi_sequences, sequence_lengths\n",
    "\n",
    "        else:\n",
    "            labels_pred = self.sess.run(self.labels_pred, feed_dict=fd)\n",
    "\n",
    "            return labels_pred, sequence_lengths\n",
    "\n",
    "\n",
    "    def run_epoch(self, train, dev, epoch):\n",
    "        \"\"\"Performs one complete pass over the train set and evaluate on dev\n",
    "\n",
    "        Args:\n",
    "            train: dataset that yields tuple of sentences, tags\n",
    "            dev: dataset\n",
    "            epoch: (int) index of the current epoch\n",
    "\n",
    "        Returns:\n",
    "            f1: (python float), score to select model on, higher is better\n",
    "\n",
    "        \"\"\"\n",
    "        # progbar stuff for logging\n",
    "        batch_size = self.config.batch_size\n",
    "        nbatches = (len(train) + batch_size - 1) // batch_size\n",
    "        prog = Progbar(target=nbatches)\n",
    "\n",
    "        # iterate over dataset\n",
    "        for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
    "            fd, _ = self.get_feed_dict(words, labels, self.config.lr,\n",
    "                    self.config.dropout)\n",
    "\n",
    "            _, train_loss, summary = self.sess.run(\n",
    "                    [self.train_op, self.loss, self.merged], feed_dict=fd)\n",
    "\n",
    "            prog.update(i + 1, [(\"train loss\", train_loss)])\n",
    "\n",
    "            # tensorboard\n",
    "            if i % 10 == 0:\n",
    "                self.file_writer.add_summary(summary, epoch*nbatches + i)\n",
    "\n",
    "        metrics = self.run_evaluate(dev)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.logger.info(msg)\n",
    "\n",
    "        return metrics[\"f1\"]\n",
    "\n",
    "\n",
    "    def run_evaluate(self, test):\n",
    "        \"\"\"Evaluates performance on test set\n",
    "\n",
    "        Args:\n",
    "            test: dataset that yields tuple of (sentences, tags)\n",
    "\n",
    "        Returns:\n",
    "            metrics: (dict) metrics[\"acc\"] = 98.4, ...\n",
    "\n",
    "        \"\"\"\n",
    "        accs = []\n",
    "        correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "        for words, labels in minibatches(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(words)\n",
    "\n",
    "            for lab, lab_pred, length in zip(labels, labels_pred,\n",
    "                                             sequence_lengths):\n",
    "                lab      = lab[:length]\n",
    "                lab_pred = lab_pred[:length]\n",
    "                accs    += [a==b for (a, b) in zip(lab, lab_pred)]\n",
    "\n",
    "                lab_chunks      = set(get_chunks(lab, self.config.vocab_tags))\n",
    "                lab_pred_chunks = set(get_chunks(lab_pred,\n",
    "                                                 self.config.vocab_tags))\n",
    "\n",
    "                correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "                total_preds   += len(lab_pred_chunks)\n",
    "                total_correct += len(lab_chunks)\n",
    "\n",
    "        p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "        r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "        f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "        acc = np.mean(accs)\n",
    "\n",
    "        return {\"acc\": 100*acc, \"f1\": 100*f1}\n",
    "\n",
    "\n",
    "    def predict(self, words_raw):\n",
    "        \"\"\"Returns list of tags\n",
    "\n",
    "        Args:\n",
    "            words_raw: list of words (string), just one sentence (no batch)\n",
    "\n",
    "        Returns:\n",
    "            preds: list of tags (string), one for each word in the sentence\n",
    "\n",
    "        \"\"\"\n",
    "        words = [self.config.processing_word(w) for w in words_raw]\n",
    "        if type(words[0]) == tuple:\n",
    "            words = zip(*words)\n",
    "        pred_ids, _ = self.predict_batch([words])\n",
    "        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
    "\n",
    "        return preds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
