{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class BaseModel(object):\n",
    "    \"\"\"Generic class for general methods that are not specific to NER\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Defines self.config and self.logger\n",
    "\n",
    "        Args:\n",
    "            config: (Config instance) class with hyper parameters,\n",
    "                vocab and embeddings\n",
    "\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.logger = config.logger\n",
    "        self.sess   = None\n",
    "        self.saver  = None\n",
    "\n",
    "\n",
    "    def reinitialize_weights(self, scope_name):\n",
    "        \"\"\"Reinitializes the weights of a given layer\"\"\"\n",
    "        variables = tf.contrib.framework.get_variables(scope_name)\n",
    "        init = tf.variables_initializer(variables)\n",
    "        self.sess.run(init)\n",
    "\n",
    "\n",
    "    def add_train_op(self, lr_method, lr, loss, clip=-1):\n",
    "        \"\"\"Defines self.train_op that performs an update on a batch\n",
    "\n",
    "        Args:\n",
    "            lr_method: (string) sgd method, for example \"adam\"\n",
    "            lr: (tf.placeholder) tf.float32, learning rate\n",
    "            loss: (tensor) tf.float32 loss to minimize\n",
    "            clip: (python float) clipping of gradient. If < 0, no clipping\n",
    "\n",
    "        \"\"\"\n",
    "        _lr_m = lr_method.lower() # lower to make sure\n",
    "\n",
    "        with tf.variable_scope(\"train_step\"):\n",
    "            if _lr_m == 'adam': # sgd method\n",
    "                optimizer = tf.train.AdamOptimizer(lr)\n",
    "            elif _lr_m == 'adagrad':\n",
    "                optimizer = tf.train.AdagradOptimizer(lr)\n",
    "            elif _lr_m == 'sgd':\n",
    "                optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            elif _lr_m == 'rmsprop':\n",
    "                optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unknown method {}\".format(_lr_m))\n",
    "\n",
    "            if clip > 0: # gradient clipping if clip is positive\n",
    "                grads, vs     = zip(*optimizer.compute_gradients(loss))\n",
    "                grads, gnorm  = tf.clip_by_global_norm(grads, clip)\n",
    "                self.train_op = optimizer.apply_gradients(zip(grads, vs))\n",
    "            else:\n",
    "                self.train_op = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "    def initialize_session(self):\n",
    "        \"\"\"Defines self.sess and initialize the variables\"\"\"\n",
    "        self.logger.info(\"Initializing tf session\")\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def restore_session(self, dir_model):\n",
    "        \"\"\"Reload weights into session\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            dir_model: dir with weights\n",
    "\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Reloading the latest trained model...\")\n",
    "        self.saver.restore(self.sess, dir_model)\n",
    "\n",
    "\n",
    "    def save_session(self):\n",
    "        \"\"\"Saves session = weights\"\"\"\n",
    "        if not os.path.exists(self.config.dir_model):\n",
    "            os.makedirs(self.config.dir_model)\n",
    "        self.saver.save(self.sess, self.config.dir_model)\n",
    "\n",
    "\n",
    "    def close_session(self):\n",
    "        \"\"\"Closes the session\"\"\"\n",
    "        self.sess.close()\n",
    "\n",
    "\n",
    "    def add_summary(self):\n",
    "        \"\"\"Defines variables for Tensorboard\n",
    "\n",
    "        Args:\n",
    "            dir_output: (string) where the results are written\n",
    "\n",
    "        \"\"\"\n",
    "        self.merged      = tf.summary.merge_all()\n",
    "        self.file_writer = tf.summary.FileWriter(self.config.dir_output,\n",
    "                self.sess.graph)\n",
    "\n",
    "\n",
    "    def train(self, train, dev):\n",
    "        \"\"\"Performs training with early stopping and lr exponential decay\n",
    "\n",
    "        Args:\n",
    "            train: dataset that yields tuple of (sentences, tags)\n",
    "            dev: dataset\n",
    "\n",
    "        \"\"\"\n",
    "        best_score = 0\n",
    "        nepoch_no_imprv = 0 # for early stopping\n",
    "        self.add_summary() # tensorboard\n",
    "\n",
    "        for epoch in range(self.config.nepochs):\n",
    "            self.logger.info(\"Epoch {:} out of {:}\".format(epoch + 1,\n",
    "                        self.config.nepochs))\n",
    "\n",
    "            score = self.run_epoch(train, dev, epoch)\n",
    "            self.config.lr *= self.config.lr_decay # decay learning rate\n",
    "\n",
    "            # early stopping and saving best parameters\n",
    "            if score >= best_score:\n",
    "                nepoch_no_imprv = 0\n",
    "                self.save_session()\n",
    "                best_score = score\n",
    "                self.logger.info(\"- new best score!\")\n",
    "            else:\n",
    "                nepoch_no_imprv += 1\n",
    "                if nepoch_no_imprv >= self.config.nepoch_no_imprv:\n",
    "                    self.logger.info(\"- early stopping {} epochs without \"\\\n",
    "                            \"improvement\".format(nepoch_no_imprv))\n",
    "                    break\n",
    "\n",
    "\n",
    "    def evaluate(self, test):\n",
    "        \"\"\"Evaluate model on test set\n",
    "\n",
    "        Args:\n",
    "            test: instance of class Dataset\n",
    "\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Testing model over test set\")\n",
    "        metrics = self.run_evaluate(test)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.logger.info(msg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
